#TODO

Nebuly开源了ChatLLaMA ，这是一个使用让我们使用自己的数据创建对话助手的框架。

![Image](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb1A2vP0l9KYMVeoQmTLdAcROSG6T6aqGbciciaPmdUXEAjJKvph9rcbFhNv1KsgtwYnuiajqJ8Iriam4g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

ChatLLaMA让我们使用自己的数据和尽可能少的计算量，来创建超个性化的类似ChatGPT的助手。

假设在未来，我们不再依赖一个「统治所有人」的大型助手，每个人都可以创建自己的个性化版本类ChatGPT助手，它们可以支持人类的各种需求。

  

![Image](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb1A2vP0l9KYMVeoQmTLdAcRQtBiahMkeKqK9mc0GGaGHaSNQIW9PevO4q3dXIB35SDN6aycA9H0HlA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

不过，创建这种个性化助手需要在许多方面做出努力：数据集创建，使用RLHF进行高效训练，以及推理优化。

这个库的目的是，通过抽象计算优化和收集大量数据所需的工作，让开发人员高枕无忧。

![Image](https://mmbiz.qpic.cn/mmbiz_png/UicQ7HgWiaUb1A2vP0l9KYMVeoQmTLdAcRaicV9ggc3bAUXlQvMTnHDlafibJLOTTC8jkItG4jQAYNAq0fiaNzBLfvQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

ChatLLaMA旨在帮助开发人员处理各种用例，所有用例都与RLHF训练和优化推理有关。以下是一些用例参考：

-   为垂直特定任务（法律、医疗、游戏、学术研究等）创建类似ChatGPT的个性化助手；
    
-   想在本地硬件基础设施上使用有限的数据，训练一个高效的类似ChatGPT的助手；
    
-   想创建自己的个性化版本类ChatGPT助手，同时避免成本失控；
    
-   想了解哪种模型架构（LLaMA、OPT、GPTJ等）最符合我在硬件、计算预算和性能方面的要求；
    
-   想让助理与我的个人/公司价值观、文化、品牌和宣言保持一致。
- 