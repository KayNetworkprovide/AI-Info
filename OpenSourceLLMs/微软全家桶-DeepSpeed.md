
github.com/microsoft/DeepSpeed
# DeepSpeed Adoption

DeepSpeed is an important part of Microsoft’s new [AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/) initiative to enable next-generation AI capabilities at scale, where you can find more information [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).

DeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):

-   [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)
-   [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)
-   [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)
-   [GLM (130B)](https://github.com/THUDM/GLM-130B)
-   [YaLM (100B)](https://github.com/yandex/YaLM-100B)
-   [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)
-   [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)
-   [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)
-   [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)
[Tutorials - DeepSpeed](https://www.deepspeed.ai/tutorials/)

![[Pasted image 20230416184515.png]]